{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "biSNbjH4JnSy"
   },
   "source": [
    "# Assignment 1\n",
    "# Sparse Bayesian Neural Networks using Automatic Relevance Determination\n",
    "\n",
    "Variational Dropout ([arXiv:1506.02557](https://arxiv.org/abs/1506.02557)) provides a Bayesian interpretation of the conventional dropout procedure. Later it was shown that Variational Dropout can be used for model sparsification (Sparse VD), an the effect can be achieved via optimization of variational lower bound wrt individual dropout rates for every weight of the model ([arXiv:1701.05369](https://arxiv.org/abs/1701.05369)).\n",
    "\n",
    "In this assignment we would consider another interpretation of this model that uses the *Automatic Relevance Determination* (ARD) prior. This model is described in [arXiv:1811.00596](https://arxiv.org/abs/1811.00596).\n",
    "\n",
    "#### ARD BNN\n",
    "\n",
    "The ARD BNN model optimizes the evidence lower bound (ELBO) $\\mathcal{L}(\\phi)$ with respect to parameters $\\phi$ of a variational approximation $q_\\phi(w)$:\n",
    "\n",
    "$$\\mathcal{L}(\\phi) =  L_\\mathcal{D}(\\phi) - D_{KL}(q_\\phi(w)\\,\\|\\,p(w\\,|\\,\\lambda^2)) \\to\\max_{\\phi, \\lambda^2},$$\n",
    "$$L_\\mathcal{D}(\\phi) = \\sum_{n=1}^N \\mathrm{E}_{q_\\phi(w)}[\\log p(y_n\\,|\\,x_n, w)],$$\n",
    "$$p(w_{ij}\\,|\\,\\lambda^2_{ij})=\\mathcal{N}(w_{ij}\\,|\\,0,\\lambda^2_{ij})\\text{ is the ARD prior,}$$\n",
    "\n",
    "$$q_\\phi(w_{ij}) = \\mathcal{N}(w_{ij}\\,|\\,\\mu_{ij},\\sigma^2_{ij})\\text{ is the fully-factorized Gaussian posterior approximation},$$\n",
    "\n",
    "and the log-likelihood $p(y\\,|\\,x, w)$ is defined by a neural network with parametrs $w$ using a conventional cross-entropy loss function. The optimization is performed by stochasic optimization methods. Adam with default hyperparameters and a simple multi step LR schedule should work fine.\n",
    "\n",
    "The optimization w.r.t. the prior variance $\\lambda^2$ can be performed analytically; the optimal value is $\\lambda_{ij}^2=\\mu_{ij}^2+\\sigma_{ij}^2$. The KL-divergence term then becomes equal to:\n",
    "\n",
    "$$D_{KL}(q_\\phi(w)\\,\\|\\,p(w\\,|\\,\\lambda^2))=\\frac12\\sum_{ij}\\log(1+\\frac{\\mu_{ij}^2}{\\sigma_{ij}^2}).$$\n",
    "\n",
    "**Note:** You will need to refer to the value $\\log\\alpha_{ij}=\\log\\frac{\\sigma_{ij}^2}{\\mu_{ij}^2}$ in order to determine which weights to zero out. It is also convenient to use it to compute the KL divergence. A rule of thumb is to zero out all the weights that have $\\log\\alpha_{ij}>3$, but you can use another threshold.\n",
    "\n",
    "**Note:** Optimize w.r.t. $\\log\\sigma$ instead of $\\sigma$ to better represent small values of $\\sigma$.\n",
    "\n",
    "**Note:** Some of the operations (e.g. log and square root) can be unstable for small arguments. In order to avoid NaNs, you can add a \"safe\" modification: just add a small constant (e.g. `1e-8`) to the argument and / or use appropriate clamping.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/bd8c169c6b162dac5aa77013d70463eae0927c5e/images/svd3.png)\n",
    "\n",
    "\n",
    "# In this assignment:\n",
    "1. Implementation of fully-connected ARD Layer\n",
    "2. Training Lenet-300-100 on MNIST dataset (test accuracy should be $\\geq 98.1$)\n",
    "3. Optional Research Assignment\n",
    "\n",
    "Additional information:\n",
    "- If you have a problem with importing logger, download logger.py and file to the same folder and run a notebook from it\n",
    "- You will need the following python packages: pytorch, numpy, sklearn, pylab (matplotlib), tabulate\n",
    "- If you have an urgent question or find a typo or a mistake, text it to Alexander Grishin in Discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NXhZgXYQJnSz"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2V-uQDOJnS2"
   },
   "source": [
    "## Implementation of  the Linear ARD layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdxU0K10JnS3"
   },
   "outputs": [],
   "source": [
    "class LinearARD(nn.Module):\n",
    "    def __init__(self, in_features, out_features, threshold, bias=True):\n",
    "        super(LinearARD, self).__init__()\n",
    "        \"\"\"\n",
    "            in_features: int, a number of input features\n",
    "            out_features: int, a number of neurons\n",
    "            threshold: float, a threshold for clipping weights\n",
    "        \"\"\"\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.mu = # torch.nn.parameter.Parameter of size out_features x in_features\n",
    "        self.log_sigma = # torch.nn.parameter.Parameter of size out_features x in_features\n",
    "        self.bias = # torch.nn.parameter.Parameter of size 1 x out_features\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.bias.data.zero_()\n",
    "        self.mu.data.normal_(0, 0.02)\n",
    "        self.log_sigma.data.fill_(-5)        \n",
    "        \n",
    "    def forward(self, x):      \n",
    "        # x is a torch.Tensor of shape (number_of_objects, in_features)\n",
    "        # log_alpha is a torch.Tensor of shape (out_features, in_features)\n",
    "        self.log_alpha = # Compute using self.log_sigma and self.mu\n",
    "        # clipping for a numerical stability\n",
    "        self.log_alpha = torch.clamp(self.log_alpha, -10, 10)   \n",
    "        \n",
    "        if self.training:\n",
    "            # LRT = local reparametrization trick\n",
    "            # lrt_mean is a torch.Tensor of shape (x.shape[0], out_features)\n",
    "            lrt_mean =  # compute mean activation using LRT; you can use F.linear\n",
    "            # lrt_std is a torch.Tensor of shape (x.shape[0], out_features)\n",
    "            lrt_std = # compute std of activations unsig lrt; you can use F.linear\n",
    "                      # do not forget use torch.sqrt(x + 1e-8) instead of torch.sqrt(x)\n",
    "            # eps is a torch.Tensor of shape (x.shape[0], out_features)\n",
    "            eps = # sample of noise for reparametrization\n",
    "            return # sample of activation\n",
    "        \n",
    "        out = # compute the output of the layer\n",
    "        # use weights W = E q = self.mu\n",
    "        # clip all weight with log_alpha > threshold\n",
    "        return out\n",
    "        \n",
    "    def kl_reg(self):\n",
    "        # kl is a scalar torch.Tensor \n",
    "        kl = # eval the KL divergence\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ps7_riOmJnS5"
   },
   "source": [
    "## Define LeNet-300-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5w7rroIeJnS7"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = LinearARD(28*28, 300, threshold)\n",
    "        self.fc2 = LinearARD(300,  100, threshold)\n",
    "        self.fc3 = LinearARD(100,  10, threshold)\n",
    "        self.threshold=threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlpR1Xr2JnS9"
   },
   "source": [
    "## Function for loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WWJNZYEJnS_"
   },
   "outputs": [],
   "source": [
    "def get_mnist(batch_size):\n",
    "    trsnform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True,\n",
    "        transform=trsnform), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeFnFEUdJnTB"
   },
   "source": [
    "## Define the ELBO loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3fVDkE1JnTC"
   },
   "outputs": [],
   "source": [
    "class ELBO(nn.Module):\n",
    "    def __init__(self, net, train_size):\n",
    "        super(ELBO, self).__init__()\n",
    "        self.train_size = train_size # int, the len of dataset\n",
    "        self.net = net # nn.Module\n",
    "        \n",
    "    def forward(self, input, target, kl_weight=1.0):\n",
    "        \"\"\"\n",
    "          input: is a torch.Tensor (a predictions of the model) \n",
    "          target: is a torch.Tensor (a tensor of labels) \n",
    "        \"\"\"\n",
    "        assert not target.requires_grad\n",
    "        kl = 0.0\n",
    "        for module in self.net.children():\n",
    "            if hasattr(module, 'kl_reg'):\n",
    "                kl = kl + module.kl_reg()\n",
    "                \n",
    "        elbo_loss = # a scalar torch.Tensor, ELBO loss\n",
    "        return elbo_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1i_Q00ESJnTF"
   },
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ROeFVbJJnTF"
   },
   "outputs": [],
   "source": [
    "model = Net(threshold=3)\n",
    "optimizer = # optimizer\n",
    "scheduler = # decrease learning rate by torch.optim.lr_scheduler\n",
    "\n",
    "logger = Logger('ard', fmt={\n",
    "    'tr_loss': '3.1e',\n",
    "    'te_loss': '3.1e',\n",
    "    'sp_0':    '.3f',\n",
    "    'sp_1':    '.3f',\n",
    "    'sp_2':    '.3f',\n",
    "    'lr':      '3.1e',\n",
    "    'kl':      '.2f',\n",
    "    'time':    '.2f',\n",
    "})\n",
    "\n",
    "train_loader, test_loader = get_mnist(batch_size=100)\n",
    "elbo = ELBO(model, len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMR2MYENJnTI"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gIcWJ2zJnTI"
   },
   "outputs": [],
   "source": [
    "# Switch the device to cuda to reduce one epoch from ~30 sec to ~10 sec\n",
    "\n",
    "device = 'cpu'\n",
    "# device = 'cuda:0'\n",
    "\n",
    "model = model.to(device)\n",
    "elbo = elbo.to(device)\n",
    "\n",
    "kl_weight = 0.02\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    time_start = time.perf_counter()\n",
    "    scheduler.step()\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0 \n",
    "    kl_weight = min(kl_weight+0.02, 1)\n",
    "    logger.add_scalar(epoch, 'kl', kl_weight)\n",
    "    logger.add_scalar(epoch, 'lr', scheduler.get_lr()[0])\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        pred = output.data.max(1)[1] \n",
    "        loss = elbo(output, target, kl_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += float(loss) \n",
    "        train_acc += np.sum(pred.cpu().numpy() == target.data.cpu().numpy())\n",
    "\n",
    "    logger.add_scalar(epoch, 'tr_loss', train_loss / len(train_loader.dataset))\n",
    "    logger.add_scalar(epoch, 'tr_acc', train_acc / len(train_loader.dataset) * 100)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        data = data.view(-1, 28*28)\n",
    "        output = model(data)\n",
    "        test_loss += float(elbo(output, target, kl_weight))\n",
    "        pred = output.data.max(1)[1] \n",
    "        test_acc += np.sum(pred.cpu().numpy() == target.data.cpu().numpy())\n",
    "        \n",
    "    logger.add_scalar(epoch, 'te_loss', test_loss / len(test_loader.dataset))\n",
    "    logger.add_scalar(epoch, 'te_acc', test_acc / len(test_loader.dataset) * 100)\n",
    "    \n",
    "    for i, c in enumerate(model.children()):\n",
    "        if hasattr(c, 'kl_reg'):\n",
    "            logger.add_scalar(epoch, 'sp_%s' % i, (c.log_alpha.data.cpu().numpy() > model.threshold).mean())\n",
    "    \n",
    "    logger.add_scalar(epoch, 'time', time.perf_counter() - time_start)\n",
    "    logger.iter_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QxpeBe-JnTN"
   },
   "outputs": [],
   "source": [
    "all_w, kep_w = 0, 0\n",
    "\n",
    "for c in model.children():\n",
    "    kep_w += (c.log_alpha.data.cpu().numpy() < model.threshold).sum()\n",
    "    all_w += c.log_alpha.data.cpu().numpy().size\n",
    "\n",
    "# compression_ratio should be > 25\n",
    "compression_ratio = all_w/kep_w\n",
    "print('compression_ratio =', compression_ratio)\n",
    "assert compression_ratio >= 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-nOBQsM6JnTR"
   },
   "source": [
    "## Disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNSNrt8UJnTT"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, csc_matrix, coo_matrix, dok_matrix\n",
    "\n",
    "row, col, data = [], [], []\n",
    "M = list(model.children())[0].mu.data.cpu().numpy()\n",
    "LA = list(model.children())[0].log_alpha.data.cpu().numpy()\n",
    "\n",
    "for i in range(300):\n",
    "    for j in range(28*28):\n",
    "        if LA[i, j] < 3:\n",
    "            row += [i]\n",
    "            col += [j]\n",
    "            data += [M[i, j]]\n",
    "\n",
    "Mcsr = csc_matrix((data, (row, col)), shape=(300, 28*28))\n",
    "Mcsc = csc_matrix((data, (row, col)), shape=(300, 28*28))\n",
    "Mcoo = coo_matrix((data, (row, col)), shape=(300, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8PKeFPLJnTV"
   },
   "outputs": [],
   "source": [
    "np.savez_compressed('M_w', M)\n",
    "scipy.sparse.save_npz('Mcsr_w', Mcsr)\n",
    "scipy.sparse.save_npz('Mcsc_w', Mcsc)\n",
    "scipy.sparse.save_npz('Mcoo_w', Mcoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpgaUhgvJnTX"
   },
   "outputs": [],
   "source": [
    "!ls -lah | grep .npz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wKMPx_8sJnTZ"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_aEHgR9UJnTb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 16, 4\n",
    "rcParams['figure.dpi'] = 200\n",
    "\n",
    "\n",
    "mask = (model.fc1.log_alpha.detach().cpu().numpy() < 3).astype(np.float)\n",
    "W = model.fc1.mu.detach().cpu().numpy()\n",
    "\n",
    "# Normalize color map\n",
    "max_val = np.max(np.abs(mask * W))\n",
    "norm = mpl.colors.Normalize(vmin=-max_val,vmax=max_val)\n",
    "\n",
    "plt.imshow(mask * W, cmap='RdBu', interpolation=None, norm=norm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBY9gsrmJnTd"
   },
   "outputs": [],
   "source": [
    "s = 0\n",
    "z = np.zeros((28*15, 28*15))\n",
    "\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        s += 1\n",
    "        z[i*28:(i+1)*28, j*28:(j+1)*28] = np.abs((mask * W)[s].reshape(28, 28))\n",
    "\n",
    "plt.figure(figsize=(24, 16))\n",
    "plt.imshow(z, cmap='hot_r')\n",
    "plt.colorbar()\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_9cgWYVJnTg"
   },
   "source": [
    "# Optional Research Assignment (up to 2 additional points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9VHbVe8JnTh"
   },
   "source": [
    "1. Study the model: \n",
    "    - How sparsity and accuracy depend on maximum of KL-multiplier (kl_weight)?\n",
    "    - How sparsity and accuracy depend on the initialization?\n",
    "    - Study the KL: what if we optimize w.r.t. $\\lambda^2$ using Adam instead of substituting the optimal $\\lambda^2$?\n",
    "    - How does the behaviour of optimization change if you use another parameterization for the approximate posterior? How does it affect the variance of the stochastic gradients?\n",
    "2. Compression:\n",
    "    - What can we do to obtain better compression results with small quality degradation?\n",
    "    - Propose and eval several options.\n",
    "3. Study the Local reparametrization trick: \n",
    "    - Does it really accelerate convergence?\n",
    "    - Does variance of gradient decrease?\n",
    "    \n",
    "You can do one out of three parts. You need to provide evidence for results e.g., plots, etc."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SparseVD-assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
